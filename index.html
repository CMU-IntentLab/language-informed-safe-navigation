<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Paper: Language Informed Safe Navigation">
  <meta property="og:title" content="Paper: Language Informed Safe Navigation" />
  <meta property="og:description"
    content="Updating Robot Safety Representations Online from Natural Language Feedback" />
  <meta property="og:url" content="https://cmu-intentlab.github.io/language-informed-safe-navigation/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/front-fig.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Paper: Language Informed Safe Navigation">
  <meta name="twitter:description"
    content="Updating Robot Safety Representations Online from Natural Language Feedback">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/front-fig.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="reachabilty navigation safety vision-language-models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Language Informed Safe Navigation</title>
  <!---
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Updating Robot Safety Representations Online from Natural Language
              Feedback</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://leohmcs.github.io/" target="_blank">Leonardo Santos</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="" target="_blank">Zirui Li</a><sup>2*</sup>,
              </span>
              <span class="author-block">
                <a href="https://lasse-peters.net" target="_blank">Lasse Peters</a><sup>3</sup>
              </span>,
              <span class="author-block">
                <a href="https://smlbansal.github.io/" target="_blank">Somil Bansal</a><sup>4†</sup>
              </span>,
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~abajcsy/" target="_blank">Andrea Bajcsy</a><sup>5†</sup>
              </span>
            </div>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block" style="font-size: 0.8em;"><sup>1</sup>UFMG, <sup>2</sup>University of Rochester, <sup>3</sup>TU Delft, <sup>4</sup>USC, <sup>5</sup>CMU</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="eql-cntrb"><small><sup>*</sup>Indicates Equal Contribution</small></span>
              <span class="eql-cntrb"><small><sup>†</sup>Indicates Equal Advising</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="TODO" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/cmu-intentlab/language-informed-safe-navigation" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!--
        <img src="static/images/front-fig.png" />
        -->
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="static/videos/banner.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <!-- TODO: add caption -->
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Robots must operate safely when deployed in novel and human-centered environments, like homes. Current
              safe control
              approaches typically assume that the safety constraints are known a priori, and thus, the robot can
              pre-compute a
              corresponding safety controller. While this may make sense for some safety constraints (e.g., avoiding
              collision with
              walls by analyzing a floor plan), other constraints are more complex (e.g., spills), inherently personal,
              context-dependent, and can only be identified at deployment time when the robot is interacting in a
              specific environment
              and with a specific person (e.g., fragile objects, expensive rugs). Here, language provides a flexible
              mechanism to
              communicate these evolving safety constraints to the robot. In this work, we use vision language models
              (VLMs) to
              interpret language feedback and the robot’s image observations to continuously update the robot’s
              representation of
              safety constraints. With these inferred constraints, we update a Hamilton-Jacobi reachability safety
              controller online
              via efficient warm-starting techniques. Through simulation and hardware experiments, we demonstrate the
              robot’s ability
              to infer and respect language-based safety constraints with the proposed approach.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Overview -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <p>
            As robots become more common in human-centered environment, we want them to be able to not only satisfy static and well-defined constraints, such as fixed obstacles, but also personalized and context-dependent safety requirements. Our key idea is to combine vision-language models for semantic constraint inference with efficient warm-started updates of a Hamilton-Jacobi Reachability-based safety controller.
          </p>
          <img src="static/images/framework.png" alt="MY ALT TEXT" class="mt-4 mb-4"/>
          <p>
            Offline, the robot has an initial failure set which encodes all physical constraints in the environment (e.g. walls, furniture that does not move), and computes the corresponding safe set and safety policy. Online, the person describes their semantic constraint. Using a vision-language model, the robot converts the language-image data into a new failure set. This, along with the previously-computed safe set, are used to efficiently update the safety filter that shields the robot.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End overview -->

  <!-- Video carousel -->
  <section class="is-light is-small">
    <div class="hero-body is-max-desktop">
      <div class="container is-centered">
        <h2 class="title is-3">Hardware Demos</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/demo_caution_tape_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/demo_coffee_spill_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/demo_dog_toys_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->

  <!-- Video carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-centered">
        <h2 class="title is-3">Baseline Comparisons</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/mppi_vlm_reachability_muted_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/mppi_vlm_muted_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/mppi_muted_with_text.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              <!-- TODO: add caption -->
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
